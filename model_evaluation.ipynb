{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean CV score: ',gs_xgb.best_score_)\n",
    "best_xgb = gs_xgb.best_estimator_ #returns the best best model, assigns it to beest_xgb\n",
    "# model prediction\n",
    "y_pred_log_xgb = best_xgb.predict(x_test) \n",
    "y_pred_xgb = np.expm1(y_pred_log_xgb)\n",
    "y_true_xgb = np.expm1(y_test)\n",
    "xgb_metrics = metrics(y_pred_log_xgb,y_test,prefix='XGB')\n",
    "xgb_metrics = metrics(y_pred_xgb,y_true_xgb,prefix='XGB')\n",
    "# If scores are consistent across folds → model generalizes well.\n",
    "# If scores vary a lot → your model is sensitive to the data (possible high variance problem)\n",
    "cv_scores = cross_val_score(best_xgb,x_train,y_train,scoring='neg_mean_squared_error',n_jobs=-1,cv=cv)\n",
    "\n",
    "cv_scores_rmse = np.sqrt(-cv_scores) #converts the neg_mean_squared_error to root mean squared error\n",
    "print('Cross validation per fold: ',cv_scores_rmse) # consistent scores mean model generalizes well - low variance\n",
    "print('Mean CV RMSE: ',cv_scores_rmse.mean()) # how much your model gets it wrong, on average\n",
    "print('Std CV RMSE: ',cv_scores_rmse.std()) # low standard deviation means model generalizes well\n",
    "\n",
    "#bias and variance trade-off\n",
    "from sklearn.model_selection import learning_curve\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_xgb,x_train,y_train,cv=5,scoring='neg_mean_squared_error',n_jobs=-1,train_sizes=np.linspace(0.1,1.0,5)\n",
    ")\n",
    "train_rmse = np.sqrt(-train_scores.mean(axis=1)) #converts mse into rmse, averages the MSE's across the 5 folds\n",
    "val_rmse = np.sqrt(-val_scores.mean(axis=1)) \n",
    "\n",
    "plt.plot(train_sizes,train_rmse,'o-',label='Training RMSE')\n",
    "plt.plot(train_sizes,val_rmse,'x-',label='Validation RMSE')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Learning Curve (Bias vs Variance trade-off)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# underfitting - curve is almost flat, training and validation errors are high\n",
    "# overfitting - training error is low, validation error is high, big gap between curves\n",
    "# good fit - both errors are low, curves are close together\n",
    "\n",
    "#high bias - underfitting (training error - high, validation error - high, learning curve - gap btwn curve is close and flat)\n",
    "# - increase model complexity_add polynomial features\n",
    "# - reduce regularization\n",
    "# - train longer \n",
    "\n",
    "#high variance - overfitting (training error - low, validation error - high, gap btwn curve is wide apart)\n",
    "# - get more training data\n",
    "# - add regularization\n",
    "# - simplify the model\n",
    "# - add cross-validation\n",
    "# - data augumentation, noise ingestion\n",
    "train_pred = best_xgb.predict(x_train) #perform predictions on training set\n",
    "val_pred = best_xgb.predict(x_test) #perform predictions on testing set\n",
    "print('Train MSE: ',mean_squared_error(y_train,train_pred))\n",
    "print('Test MSE: ',mean_squared_error(y_test,val_pred))\n",
    "\n",
    "# if both train and test MSE are close but very large  numbers - underfitting\n",
    "# if test MSE is way larger than train MSE - overfitting\n",
    "# if both train and test MSE are close with small numebrs - good fit\n",
    "# residuals - difference between predicted and actual values\n",
    "residuals = y_test - val_pred\n",
    "\n",
    "# residual plots\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.scatterplot(x=val_pred,y=residuals)\n",
    "plt.axhline(0,color='red',linestyle='--')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual plots')\n",
    "plt.show()\n",
    "# ideal if residuals look like noise around 0\n",
    "# systematic patterns equals bad model\n",
    "# bigger errors around edges means model struggles with extreme values\n",
    "#error distribution\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.histplot(residuals,kde=True)\n",
    "plt.axvline(color='red',linestyle='--')\n",
    "plt.title('Distribution of residuals')\n",
    "plt.show()\n",
    "\n",
    "# Good model if error is centered around 0\n",
    "# bell curved shape equals good model\n",
    "#error segmentation\n",
    "results = pd.DataFrame({\n",
    "    'y_true' : y_true_xgb,\n",
    "    'y_pred' : y_pred_xgb\n",
    "})\n",
    "results['fuel'] = x_test['fuel']\n",
    "results['owner'] = x_test['owner']\n",
    "results['transmission'] = x_test['transmission']\n",
    "results['mileage'] = x_test['mileage']\n",
    "results['seats'] = x_test['seats']\n",
    "\n",
    "eps = 1e-8\n",
    "results['residual'] = results['y_true'] - results['y_pred']\n",
    "results['abs_error'] = results['residual'].abs()\n",
    "results['squared_error'] = results['residual']**2\n",
    "results['ape'] = (results['abs_error']/np.maximum(results['y_true'],eps)) * 100.0\n",
    "\n",
    "fuel_seg = (\n",
    "    results.groupby('fuel',observed=True)\n",
    "    .agg(\n",
    "        n = ('y_true','size'),\n",
    "        mae = ('abs_error','mean'),\n",
    "        rmse = ('squared_error',lambda s: np.sqrt(s.mean())),\n",
    "        mape = ('ape','median'),\n",
    "        true_median = ('y_true','median')\n",
    "    )\n",
    "    .query('n >= 30')\n",
    "    .sort_values('mae',ascending=False)\n",
    ")\n",
    "print(\"Top brands by MAE:\\n\", fuel_seg.head(10), \"\\n\")\n",
    "#robustness and stress testing\n",
    "x_test_noisy = x_test.copy()\n",
    "numeric = x_test_noisy.select_dtypes(include='number').columns\n",
    "noise = np.random.normal(0,0.1,x_test_noisy[numeric].shape)\n",
    "x_test_noisy[numeric] = x_test_noisy[numeric] + noise\n",
    "\n",
    "x_test_noisy_pred = best_xgb.predict(x_test_noisy)\n",
    "\n",
    "print('MSE for original prediction: ', mean_squared_error(y_test,y_pred_log_xgb))\n",
    "print('MSE for noisy prediction : ',mean_squared_error(y_test,x_test_noisy_pred))\n",
    "\n",
    "#small changes in model after adding noise - good robustness\n",
    "#big changes in model after adding noise - fragile model\n",
    "pipe_rf = Pipeline(steps=[\n",
    "    ('preprocessor',preprocessor),\n",
    "    ('model',RandomForestRegressor(random_state=42,n_jobs=1))\n",
    "])\n",
    "grid_params = {\n",
    "    'model__n_estimators' : [50,100],\n",
    "    'model__max_depth' : [None,20,25],\n",
    "    'model__min_samples_split' : [2,5],\n",
    "    'model__max_features' : ['sqrt',0.5],\n",
    "    'preprocessor__num__poly__degree' : [1,2,3]\n",
    "}\n",
    "gs_rf = GridSearchCV(pipe_rf,grid_params,cv=cv,n_jobs=-1,verbose=2,return_train_score=True, refit=True)\n",
    "print('Fitting Gridsearch for Random Forest (this may take a while) ... ')\n",
    "gs_rf.fit(x_train,y_train)\n",
    "print('\\nBest params : ',gs_rf.best_params_)\n",
    "print('\\nBest CV Score: ',gs_rf.best_score_)\n",
    "best_rf = gs_rf.best_estimator_\n",
    "y_pred_rf = best_rf.predict(x_test)\n",
    "print('\\nRandom forest performance on test set')\n",
    "rf_metrics = metrics(y_test,y_pred_rf,prefix='RF')\n",
    "cv_scores = cross_val_score(best_rf, x_train, y_train, cv=cv,scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "cv_rmse = np.sqrt(-cv_scores)\n",
    "print(f'CV_RMSE mean score: {cv_rmse.mean():.2f} (+/- {cv_rmse.std():.2f})')\n",
    "cv_results_rf = pd.DataFrame(gs_rf.cv_results_).sort_values('mean_test_score',ascending=False)\n",
    "print('Top RF CV results (Top 5) ...')\n",
    "print(cv_results_rf[['params','mean_test_score','std_test_score']].head())jjjjjjj"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
